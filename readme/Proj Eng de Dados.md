# Proj. Eng. de Dados

# **Desafio**

*Neste desafio voc√™ dever√° realizar a extra√ß√£o e carregamento dos dados de ambas as fontes fornecidas (CSV e SQL) para um Data Warehouse **local** (PostgreSQL). Essa tarefa dever√° respeitar os requisitos e diagrama abaixo:*

![image.png](image.png)

### **Requisitos**

- Utilize o Apache Airflow (2 ou 3) como orquestrador de tarefas; - instalar no WSL
- As extra√ß√µes devem ser idempotentes ‚Üí ou seja, o c√≥digo tem que retornar a mesma sa√≠da todas as vezes.
- Devem ser extra√≠dos todos os dados fornecidos;
- As extra√ß√µes devem escrever os dados no formato CSV para seu FIleSystem Local seguindo o padr√£o de nomenclatura:
    - <ano>-<m√™s>-<dia>/<fonte-de-dados>/<nome-da-tabela-ou-csv>.csv
- As etapas de extra√ß√£o de dados devem ocorrer uma em paralelo √† outra;
- A etapa de carregamento no Data Warehouse deve ocorrer somente se ambas extra√ß√µes tenham sucesso;
- O pipeline deve ser executado todos os dias √†s 04:35 da manh√£;
- O projeto deve ser reproduz√≠vel em outros ambientes.

# **Entregas**

1. Todos os c√≥digos e configura√ß√µes utilizados para a execu√ß√£o do projeto devem ser compactados num arquivo **.zip** e anexados na entrega.
2. Um documento descritivo com instru√ß√µes para execu√ß√£o do projeto em outros ambientes.
3. Um **v√≠deo** curto explicando a l√≥gica utilizada em sua DAG do Airflow bem como uma execu√ß√£o manual da mesma e os resultados obtidos a partir da execu√ß√£o do pipeline (arquivos escritos no formato desejado e dados adicionados ao Data Warehouse).

AT√â QUINTA FEIRA

# Notas pessoais sobre o projeto

## 1. Arquivo CSV - transacoes.csv

A coluna de datas que foi a base para a constru√ß√£o dos diret√≥rios possui datas com formato misto ‚Üí alguns registros tem milissegundos, outras n√£o. O que impedia a manipula√ß√£o inicial para cria√ß√£o dos diret√≥rios. 

Corrigido com o:

```python
df['data_transacao'] = pd.to_datetime(df['data_transacao'], utc=True, format="mixed")
```

A partir disso, os diret√≥rios recursivos foram criados sem problemas. 

## 2. Arquivo SQL - banvic.sql

O trabalho que o .csv n√£o deu, esse deu de sobra. 

1. N√£o consegui importar os dados direto do arquivo .sql; provavelmente algum conflito da minha vers√£o do Postgres Vs. a vers√£o do banco ‚Äúoriginal‚Äù.
2. Fu√ßando o arquivo, vi que dentre os dados dos clientes, havia um endere√ßo com ‚Äòaspas simples‚Äô, o que deixava um bloco dos dados como ‚Äòstring‚Äô.

![image.png](image%201.png)

Um bloco bem grandinho diga-se de passagem. 

1. A senha com caractere especial

O sqlalchemy, lib usada para manipular esses dados, n√£o conseguia interpretar a senha por causa do *‚Äò&‚Äô*. Resolvido com: 

```python
#imbutido nos c√≥digos
password_raw = "v3rysecur&pas5w0rd"
encoded_password = quote_plus(password_raw)
connection_str = f"postgresql://data_engineer:{encoded_password}@localhost:5432/banvic"

conn = psycopg2.connect(connection_str)
```

1. A forma como foi feito o import dos dados para o DB

A utilidade do arquivo .sql foi basicamente pra ‚Äúrestaurar‚Äù a estrutura das tabelas e suas rela√ß√µes. Os dados dos arquivos foram importados de .csvs correspondentes as mesmas.

![image.png](image%202.png)

![image.png](image%203.png)

Ao todo, s√£o 46 colunas. Quando ‚Äúmergedas‚Äù, 33. Est√£o organizadas por ordem de relev√¢ncia.

![image.png](image%204.png)

1. A cria√ß√£o dos diret√≥rios

Uma vez que chegou-se ao DB com as tabelas/colunas, foi xuxu beleza pra criar os diret√≥rios. Bastou reutilizar o c√≥digo usado pro CSV e adptar para o SQL (que na verdade agora √© um CSV tamb√©m üòõ).

![image.png](image%205.png)

Diferente do CSV, o banvic.sql tem 5 colunas de datas: *data_inclusao, data_nascimento, data_abertura_conta, data_entrada_proposta* e **data_ultimo_lancamento**, sendo essa √∫ltima a utilizada para a cria√ß√£o dos diret√≥rios. Era a de maior peso para a constru√ß√£o dos mesmos.